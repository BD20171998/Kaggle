{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#This Python 3 environment comes with many helpful analytics libraries installed\n#It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n#For example, here's several helpful packages to load\n\n#https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\nimport re\nimport nltk\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nnltk.download('punkt')\n\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords,wordnet\n\n\nimport fasttext\n\n\n#Input data files are available in the read-only \"../input/\" directory\n#For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n#Splitting data into training and test sets\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntrain_set = train.sample(frac=0.75, random_state=0)\ntest_set = train.drop(train_set.index)\n\n#Tokenize tweets\ntt = TweetTokenizer()\ntrain_set['raw_tokens']=train_set['text'].apply(tt.tokenize)\n\n\n#Preprocessing data\nf=lambda x: [re.sub('[^a-zA-Z]', '', y) for y in x]\ntrain_set['cleaned_tokens'] = train_set['raw_tokens'].apply(f)\n\n\n#Convert to lower case and remove stopwords\nstopword_set = set(stopwords.words(\"english\"))\nj = lambda x:[y.lower() for y in list(x) if y.lower() not in stopword_set and y != '']\ntrain_set['cleaned_tokens'] = train_set['cleaned_tokens'].apply(j)\n\n\nl= lambda x: ' '.join(word for word in x)\n\ntrain_set['cleaned_tokens'] = train_set['cleaned_tokens'].apply(l)\n\n\n#drop column data no longer needed\ntrain_set=train_set.drop(['raw_tokens'], axis=1)\n\n#Prepping traing data for fastText model\nft = lambda x: '__label__'+ str(x['target'])+ ' '+x['cleaned_tokens']\ntrain_set['fast_text']=train_set.apply(ft, axis=1)\nnp.savetxt(r'fasttext.txt', train_set['fast_text'],fmt='%s')\n\n\n#prepping test set data for model\ntest_set['raw_tokens']=test_set['text'].apply(tt.tokenize)\ntest_set['cleaned_tokens'] = test_set['raw_tokens'].apply(f)\ntest_set['cleaned_tokens'] = test_set['cleaned_tokens'].apply(j)\ntest_set['cleaned_tokens'] = test_set['cleaned_tokens'].apply(l)\n\n#Function to return predicted data from model\ndef get_prediction(x):\n    x = x.split('__label__')\n    return(int(x[1]))\n\n\n#10-epoch models using 10,5,3,& 2 N grams respectively\nmodel_1_10epochs = fasttext.train_supervised(input='fasttext.txt',lr=1.0,epoch=10,wordNgrams=10)\nmodel_2_10epochs = fasttext.train_supervised(input='fasttext.txt',lr=1.0,epoch=10,wordNgrams=5)\nmodel_3_10epochs = fasttext.train_supervised(input='fasttext.txt',lr=1.0,epoch=10,wordNgrams=3)\nmodel_4_10epochs = fasttext.train_supervised(input='fasttext.txt',lr=1.0,epoch=10,wordNgrams=2)\n\n\n#Retreiving accuracy precentages for 10-epochs models\nx1=lambda y: model_1_10epochs.predict(y)[0][0]\nx2=lambda y: model_2_10epochs.predict(y)[0][0]\nx3=lambda y: model_3_10epochs.predict(y)[0][0]\nx4=lambda y: model_4_10epochs.predict(y)[0][0]\n\ntest_set['prediction1']=test_set['cleaned_tokens'].apply(x1)\ntest_set['prediction1']=test_set['prediction1'].apply(get_prediction)\np1=len(test_set[test_set['prediction1']==test_set['target']])/len(test_set['target'])\n\ntest_set['prediction2']=test_set['cleaned_tokens'].apply(x2)\ntest_set['prediction2']=test_set['prediction2'].apply(get_prediction)\np2=len(test_set[test_set['prediction2']==test_set['target']])/len(test_set['target'])\n\ntest_set['prediction3']=test_set['cleaned_tokens'].apply(x3)\ntest_set['prediction3']=test_set['prediction3'].apply(get_prediction)\np3=len(test_set[test_set['prediction3']==test_set['target']])/len(test_set['target'])\n\ntest_set['prediction4']=test_set['cleaned_tokens'].apply(x4)\ntest_set['prediction4']=test_set['prediction4'].apply(get_prediction)\np4=len(test_set[test_set['prediction4']==test_set['target']])/len(test_set['target'])\n\nprint(p1,p2,p3,p4)\n\n\n#20-epoch models using 10,5,3,& 2 N grams respectively\nmodel_1_20epochs = fasttext.train_supervised(input='fasttext.txt',lr=1.0,epoch=20,wordNgrams=10)\nmodel_2_20epochs = fasttext.train_supervised(input='fasttext.txt',lr=1.0,epoch=20,wordNgrams=5)\nmodel_3_20epochs = fasttext.train_supervised(input='fasttext.txt',lr=1.0,epoch=20,wordNgrams=3)\nmodel_4_20epochs = fasttext.train_supervised(input='fasttext.txt',lr=1.0,epoch=20,wordNgrams=2)\n\n\n#Retreiving accuracy precentages for 20-epoch models\nx5=lambda y: model_1_20epochs.predict(y)[0][0]\nx6=lambda y: model_2_20epochs.predict(y)[0][0]\nx7=lambda y: model_3_20epochs.predict(y)[0][0]\nx8=lambda y: model_4_20epochs.predict(y)[0][0]\n\ntest_set['prediction5']=test_set['cleaned_tokens'].apply(x5)\ntest_set['prediction5']=test_set['prediction5'].apply(get_prediction)\np5=len(test_set[test_set['prediction5']==test_set['target']])/len(test_set['target'])\n\ntest_set['prediction6']=test_set['cleaned_tokens'].apply(x6)\ntest_set['prediction6']=test_set['prediction6'].apply(get_prediction)\np6=len(test_set[test_set['prediction6']==test_set['target']])/len(test_set['target'])\n\ntest_set['prediction7']=test_set['cleaned_tokens'].apply(x7)\ntest_set['prediction7']=test_set['prediction7'].apply(get_prediction)\np7=len(test_set[test_set['prediction7']==test_set['target']])/len(test_set['target'])\n\ntest_set['prediction8']=test_set['cleaned_tokens'].apply(x8)\ntest_set['prediction8']=test_set['prediction8'].apply(get_prediction)\np8=len(test_set[test_set['prediction8']==test_set['target']])/len(test_set['target'])\n\nprint(p5,p6,p7,p8)\n\n\n#100-epoch models using 10,5,3,& 2 N grams respectively\nmodel_1_100epochs = fasttext.train_supervised(input='fasttext.txt',lr=1.0,epoch=100,wordNgrams=10)\nmodel_2_100epochs = fasttext.train_supervised(input='fasttext.txt',lr=1.0,epoch=100,wordNgrams=5)\nmodel_3_100epochs = fasttext.train_supervised(input='fasttext.txt',lr=1.0,epoch=100,wordNgrams=3)\nmodel_4_100epochs = fasttext.train_supervised(input='fasttext.txt',lr=1.0,epoch=100,wordNgrams=2)\n\n\n#Retreiving accuracy precentages for 100-epoch models\nx9=lambda y: model_1_100epochs.predict(y)[0][0]\nx10=lambda y: model_2_100epochs.predict(y)[0][0]\nx11=lambda y: model_3_100epochs.predict(y)[0][0]\nx12=lambda y: model_4_100epochs.predict(y)[0][0]\n\ntest_set['prediction9']=test_set['cleaned_tokens'].apply(x9)\ntest_set['prediction9']=test_set['prediction9'].apply(get_prediction)\np9=len(test_set[test_set['prediction9']==test_set['target']])/len(test_set['target'])\n\ntest_set['prediction10']=test_set['cleaned_tokens'].apply(x10)\ntest_set['prediction10']=test_set['prediction10'].apply(get_prediction)\np10=len(test_set[test_set['prediction10']==test_set['target']])/len(test_set['target'])\n\ntest_set['prediction11']=test_set['cleaned_tokens'].apply(x11)\ntest_set['prediction11']=test_set['prediction11'].apply(get_prediction)\np11=len(test_set[test_set['prediction11']==test_set['target']])/len(test_set['target'])\n\ntest_set['prediction12']=test_set['cleaned_tokens'].apply(x12)\ntest_set['prediction12']=test_set['prediction12'].apply(get_prediction)\np12=len(test_set[test_set['prediction12']==test_set['target']])/len(test_set['target'])\n\nprint(p9,p10,p11,p12)\n\n#Retreiving accuracy precentages for epochs at 1000 using various n-grams\nx13=lambda y: model_1_100epochs.predict(y)[0][0]\nx14=lambda y: model_2_100epochs.predict(y)[0][0]\nx15=lambda y: model_3_100epochs.predict(y)[0][0]\nx16=lambda y: model_4_100epochs.predict(y)[0][0]\n\ntest_set['prediction13']=test_set['cleaned_tokens'].apply(x13)\ntest_set['prediction13']=test_set['prediction13'].apply(get_prediction)\np13=len(test_set[test_set['prediction13']==test_set['target']])/len(test_set['target'])\n\ntest_set['prediction14']=test_set['cleaned_tokens'].apply(x14)\ntest_set['prediction14']=test_set['prediction14'].apply(get_prediction)\np14=len(test_set[test_set['prediction14']==test_set['target']])/len(test_set['target'])\n\ntest_set['prediction15']=test_set['cleaned_tokens'].apply(x15)\ntest_set['prediction15']=test_set['prediction15'].apply(get_prediction)\np15=len(test_set[test_set['prediction15']==test_set['target']])/len(test_set['target'])\n\ntest_set['prediction16']=test_set['cleaned_tokens'].apply(x16)\ntest_set['prediction16']=test_set['prediction16'].apply(get_prediction)\np16=len(test_set[test_set['prediction16']==test_set['target']])/len(test_set['target'])\n\nprint(p13,p14,p15,p16)\n\n#You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n#You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-08-07T00:45:07.929242Z","iopub.execute_input":"2021-08-07T00:45:07.930019Z","iopub.status.idle":"2021-08-07T00:45:27.251126Z","shell.execute_reply.started":"2021-08-07T00:45:07.929918Z","shell.execute_reply":"2021-08-07T00:45:27.249956Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n0.7955859169732002 0.7997898055701524 0.7961114030478192 0.8008407777193904\n","output_type":"stream"}]}]}